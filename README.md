## What Does Transformer look at?
A repository that contains code and results for visualizing Attention and analyzing it using the some transformer model
You also can calculate the diff attention using norms between same length the sentence.

The following papers and Github repositories are referenced for the use of norm calculations  
[github](https://github.com/gorokoba560/norm-analysis-of-transformer)  
[paper](https://www.anlp.jp/proceedings/annual_meeting/2021/pdf_dir/A7-2.pdf)

### Target models
BERT model: https://huggingface.co/cl-tohoku/bert-base-japanese<br>
